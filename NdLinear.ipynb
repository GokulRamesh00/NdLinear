{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f42298dd-42ca-406e-b49f-3a9616b2fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e651f60-8da3-4fd8-8bfc-0b184174e5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfe40c0c-77db-4ea4-8b3a-8c1d7f0a8651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 160 images across 16 classes.\n"
     ]
    }
   ],
   "source": [
    "# Sample a smaller dataset from full data\n",
    "FULL_CSV = \"data/labels.csv\"\n",
    "FULL_IMG_DIR = \"data/images\"\n",
    "SAMPLE_CSV = \"data/labels_sample.csv\"\n",
    "SAMPLE_IMG_DIR = \"data/images_sample\"\n",
    "\n",
    "os.makedirs(SAMPLE_IMG_DIR, exist_ok=True)\n",
    "\n",
    "original_df = pd.read_csv(FULL_CSV)\n",
    "sampled_data = []\n",
    "label_counts = {}\n",
    "SAMPLE_SIZE = 10  # number of images per class\n",
    "\n",
    "for row in original_df.itertuples(index=False):\n",
    "    count = label_counts.get(row.label, 0)\n",
    "    if count < SAMPLE_SIZE:\n",
    "        src = os.path.join(FULL_IMG_DIR, row.image_name)\n",
    "        dst = os.path.join(SAMPLE_IMG_DIR, row.image_name)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "            sampled_data.append({\"image_name\": row.image_name, \"label\": row.label})\n",
    "            label_counts[row.label] = count + 1\n",
    "\n",
    "sample_df = pd.DataFrame(sampled_data)\n",
    "sample_df.to_csv(SAMPLE_CSV, index=False)\n",
    "print(f\"Sampled {len(sample_df)} images across {len(set(sample_df.label))} classes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b58ce2fe-3bd9-4a60-9ac9-9cac8a224160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply OCR and Split\n",
    "ocr_data = []\n",
    "for row in sample_df.itertuples(index=False):\n",
    "    img_path = os.path.join(SAMPLE_IMG_DIR, row.image_name)\n",
    "    try:\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        text = pytesseract.image_to_string(img).strip()\n",
    "        ocr_data.append({\"image_name\": row.image_name, \"ocr_text\": text, \"label\": row.label})\n",
    "    except Exception as e:\n",
    "        print(f\"Failed OCR on {row.image_name}: {e}\")\n",
    "\n",
    "ocr_df = pd.DataFrame(ocr_data)\n",
    "ocr_df.dropna(subset=[\"ocr_text\"], inplace=True)  # clean\n",
    "ocr_df.to_csv(\"data/all_sample_data.csv\", index=False)\n",
    "\n",
    "train_df, test_df = train_test_split(ocr_df, test_size=0.2, stratify=ocr_df[\"label\"], random_state=42)\n",
    "train_df.to_csv(\"data/train_sample.csv\", index=False)\n",
    "test_df.to_csv(\"data/test_sample.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f8a666f-0c10-4dc5-bae0-9bd037fa3d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and Model\n",
    "import re\n",
    "\n",
    "# Function to clean OCR text\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.,!? ]\", \" \", text)  # Keep basic punctuation and alphanumerics\n",
    "    return text.strip()\n",
    "    \n",
    "class DocumentDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, tokenizer, transform):\n",
    "        self.df = dataframe\n",
    "        self.img_dir = img_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(os.path.join(self.img_dir, row['image_name'])).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "\n",
    "        text = str(row['ocr_text']) if pd.notna(row['ocr_text']) else \"\"\n",
    "        tokens = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'image': img,\n",
    "            'input_ids': tokens['input_ids'].squeeze(0),\n",
    "            'attention_mask': tokens['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(row['label'])\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "class NdLinear(nn.Module):\n",
    "    def __init__(self, in_shape, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(np.prod(in_shape), out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.linear(x)\n",
    "\n",
    "class MultiModalClassifier(nn.Module):\n",
    "    def __init__(self, ndlinear_dim=256, num_classes=16):\n",
    "        super().__init__()\n",
    "        self.image_encoder = models.resnet18(pretrained=True)\n",
    "        self.image_encoder.fc = nn.Identity()\n",
    "        self.text_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.ndlinear = NdLinear((2, 768), ndlinear_dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ndlinear_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        img_feat = self.image_encoder(image)\n",
    "        txt_feat = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "        if img_feat.shape[1] != txt_feat.shape[1]:\n",
    "            img_feat = F.pad(img_feat, (0, txt_feat.shape[1] - img_feat.shape[1]))\n",
    "        combined = torch.stack([txt_feat, img_feat], dim=1)\n",
    "        fused = self.ndlinear(combined)\n",
    "        return self.classifier(fused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d66eed0-a50d-4f3c-b894-ac3ae03a0072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gokul\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gokul\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 2.7723, Accuracy = 0.0703\n",
      "Epoch 2: Loss = 2.7149, Accuracy = 0.1875\n",
      "Epoch 3: Loss = 2.6640, Accuracy = 0.2578\n",
      "Epoch 4: Loss = 2.5794, Accuracy = 0.4062\n",
      "Epoch 5: Loss = 2.4730, Accuracy = 0.5000\n",
      "Epoch 6: Loss = 2.3705, Accuracy = 0.4922\n",
      "Epoch 7: Loss = 2.2346, Accuracy = 0.6172\n",
      "Epoch 8: Loss = 2.1060, Accuracy = 0.6875\n",
      "Epoch 9: Loss = 1.9486, Accuracy = 0.7656\n",
      "Epoch 10: Loss = 1.8055, Accuracy = 0.7891\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "SAMPLE_IMG_DIR = \"data/images_sample\"\n",
    "train_data = pd.read_csv(\"data/train_sample.csv\")\n",
    "test_data = pd.read_csv(\"data/test_sample.csv\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "\n",
    "train_dataset = DocumentDataset(train_data, SAMPLE_IMG_DIR, tokenizer, transform)\n",
    "test_dataset = DocumentDataset(test_data, SAMPLE_IMG_DIR, tokenizer, transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4)\n",
    "\n",
    "model = MultiModalClassifier(num_classes=len(sample_df.label.unique())).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    total_loss, correct = 0, 0\n",
    "    for batch in train_loader:\n",
    "        image = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image, input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss/len(train_loader):.4f}, Accuracy = {correct/len(train_dataset):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e7050d3-e998-424b-8239-d03f0862332f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      1.00      0.57         2\n",
      "           1       0.50      1.00      0.67         2\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       1.00      0.50      0.67         2\n",
      "           6       0.00      0.00      0.00         2\n",
      "           7       0.00      0.00      0.00         2\n",
      "           8       0.33      1.00      0.50         2\n",
      "           9       0.00      0.00      0.00         2\n",
      "          10       0.00      0.00      0.00         2\n",
      "          11       0.00      0.00      0.00         2\n",
      "          12       0.33      0.50      0.40         2\n",
      "          13       0.33      1.00      0.50         2\n",
      "          14       0.00      0.00      0.00         2\n",
      "          15       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.38        32\n",
      "   macro avg       0.24      0.38      0.27        32\n",
      "weighted avg       0.24      0.38      0.27        32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gokul\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\gokul\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\gokul\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Model\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        image = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(image, input_ids, attention_mask)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7368de-6c80-409b-bba8-4eb391bdd4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
