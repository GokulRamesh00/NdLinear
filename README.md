# Multi-Modal Document Classification with NdLinear

This project builds a multimodal deep learning model to classify scanned document images by combining both visual (image) and textual (OCR-extracted) features. The model leverages a ResNet backbone for image encoding, BERT for text encoding, and a custom `NdLinear` layer to fuse both modalities effectively.

---

## ğŸ“Œ Key Features

- âœ… **Multimodal Fusion**: Combines visual (ResNet18) and text (BERT) features
- âœ… **NdLinear Layer**: Replaces traditional FC layers with a more expressive fusion method
- âœ… **OCR Integration**: Extracts text using Tesseract OCR from `.tif`, `.jpg`, etc.
- âœ… **Small Sample-Friendly**: Trains even on low-resource, balanced sample datasets
- âœ… **Flexible Input**: Works on folder-structured datasets (1 folder = 1 class)

---


## ğŸ“¦ Dataset

The dataset used in this project is derived from the publicly available **RVL-CDIP** dataset on Kaggle.

ğŸ“‚ **Kaggle Dataset:**  
[The RVL-CDIP Dataset (Test)](https://www.kaggle.com/datasets/pdavpoojan/the-rvlcdip-dataset-test?resource=download)

## Model Architecture

```text
                 +----------------+       +----------------+
   image input â†’ |   ResNet18     |       |     BERT       | â† ocr_text
                 +--------+-------+       +--------+-------+
                          |                        |
                          +----------+-------------+
                                     â†“
                            [ ResNet â¬Œ BERT ]
                               via NdLinear
                                     â†“
                          Fully Connected Head
                                     â†“
                             Class Probabilities
```

## ğŸªª License

This project is open-sourced under the MIT License.

